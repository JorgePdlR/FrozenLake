{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 478,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "\n",
    "# Configures numpy print options\n",
    "@contextlib.contextmanager\n",
    "def _printoptions(*args, **kwargs):\n",
    "    original = np.get_printoptions()\n",
    "    np.set_printoptions(*args, **kwargs)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.set_printoptions(**original)\n",
    "\n",
    "\n",
    "class EnvironmentModel:\n",
    "    def __init__(self, n_states, n_actions, seed=None):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "\n",
    "    def p(self, next_state, state, action):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def r(self, next_state, state, action):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def draw(self, state, action):\n",
    "        p = [self.p(ns, state, action) for ns in range(self.n_states)]\n",
    "        next_state = self.random_state.choice(self.n_states, p=p)\n",
    "        reward = self.r(next_state, state, action)\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "class Environment(EnvironmentModel):\n",
    "    def __init__(self, n_states, n_actions, max_steps, pi, seed=None):\n",
    "        EnvironmentModel.__init__(self, n_states, n_actions, seed)\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.pi = pi\n",
    "        if self.pi is None:\n",
    "            self.pi = np.full(n_states, 1. /n_states)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_steps = 0\n",
    "        self.state = self.random_state.choice(self.n_states, p=self.pi)\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= self.n_actions:\n",
    "            raise Exception('Invalid action.')\n",
    "\n",
    "        self.n_steps += 1\n",
    "        done = (self.n_steps >= self.max_steps)\n",
    "\n",
    "        self.state, reward = self.draw(self.state, action)\n",
    "\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def render(self, policy=None, value=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class FrozenLake(Environment):\n",
    "    def __init__(self, lake, slip, max_steps, seed=None):\n",
    "        \"\"\"\n",
    "        lake: A matrix that represents the lake. For example:\n",
    "        lake =  [['&', '.', '.', '.'],\n",
    "                ['.', '#', '.', '#'],\n",
    "                ['.', '.', '.', '#'],\n",
    "                ['#', '.', '.', '$']]\n",
    "        slip: The probability that the agent will slip\n",
    "        max_steps: The maximum number of time steps in an episode\n",
    "        seed: A seed to control the random number generator (optional)\n",
    "        \"\"\"\n",
    "        # start (&), frozen (.), hole (#), goal ($)\n",
    "        self.lake = np.array(lake)\n",
    "        self.lake_flat = self.lake.reshape(-1)\n",
    "\n",
    "        self.slip = slip\n",
    "\n",
    "        n_states = self.lake.size + 1\n",
    "        n_actions = 4\n",
    "\n",
    "        pi = np.zeros(n_states, dtype=float)\n",
    "        pi[np.where(self.lake_flat == '&')[0]] = 1.0\n",
    "\n",
    "        self.absorbing_state = n_states - 1\n",
    "\n",
    "        # TODO: We can pre-calculate p and r here and put them in an array but is not necessary as answered in the forum\n",
    "\n",
    "        Environment.__init__(self, n_states, n_actions, max_steps, pi, seed=seed)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done = Environment.step(self, action)\n",
    "\n",
    "        done = (state == self.absorbing_state) or done\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def p(self, next_state, state, action):\n",
    "        # Convert possible actions to directions for clarity of reading the code:\n",
    "        up, left, down, right = [0, 1, 2, 3]\n",
    "        # Probability of transitioning from state to next_state given an action\n",
    "        # (return value)\n",
    "        pt = 0.0\n",
    "        \n",
    "        \n",
    "        # 1. If the agent is in the absorbing state the probability of remaining in \n",
    "        #    the same place is 1. Any action taken in the absorbing state leads\n",
    "        #    to the absorbing state. Validating this first to avoid dealing with\n",
    "        #    'out of index' issues with the self.lake_flat array since the\n",
    "        #    absorbing state is in the self.lake.size + 1 position\n",
    "        if state == self.absorbing_state and next_state == self.absorbing_state:\n",
    "            # Assigning return value to pt just to keep the convention that pt\n",
    "            # is the return value. Same convention is used in the rest of the\n",
    "            # function.\n",
    "            pt = 1.0\n",
    "            return pt\n",
    "        elif state == self.absorbing_state:\n",
    "            return pt\n",
    "   \n",
    "        # Is the agent in a hole or the goal ?\n",
    "        hole_or_goal = True if self.lake_flat[state] == '#' or self.lake_flat[state] == '$' else False\n",
    "\n",
    "\n",
    "        # 2. If the agent is in a hole or in the goal the probability of moving to \n",
    "        #    the absorbing state is 1.\n",
    "        if hole_or_goal and next_state == self.absorbing_state:\n",
    "            pt = 1.0\n",
    "            return pt\n",
    "        # If the agent is in any position different from a hole or the goal, and it\n",
    "        # wants to move to the absorbing state the probability is 0, or if it is\n",
    "        # in a hole or the goal, and it tries to move to any other state different\n",
    "        # from the absorbing state the probability is 0. \n",
    "        elif next_state == self.absorbing_state or hole_or_goal:\n",
    "            return pt\n",
    "        \n",
    "        \n",
    "        # 3. Validate if the agent can move from the current state to the next_state.\n",
    "        #    Consider that it has 4 possible actions: up, down, left, right;\n",
    "        #    this means that it can move in just 1 direction per action, either\n",
    "        #    x or y. The absolute change in the coordinates x, y from state to\n",
    "        #    next_state can't be greater than 1\n",
    "        \n",
    "        # Get x, y coordinates of state and next_state \n",
    "        next_state_y, next_state_x = np.unravel_index(next_state, self.lake.shape)\n",
    "        state_y, state_x = np.unravel_index(state, self.lake.shape)\n",
    "        # A negative delta_x indicates that the agent should move to the left,\n",
    "        # positive delta_x indicates it should move to the right\n",
    "        delta_x = next_state_x - state_x\n",
    "        # A negative delta_y indicates that the agent should go down, positive\n",
    "        # delta_y indicate it should go up\n",
    "        delta_y = next_state_y - state_y\n",
    "\n",
    "        # Probability of making an invalid move is 0\n",
    "        if (np.abs(delta_x) + np.abs(delta_y)) > 1 and not hole_or_goal:\n",
    "            return pt\n",
    "        \n",
    "        \n",
    "        # 4. Given that the transition from state to next_state is valid, validate\n",
    "        #    that by executing the action the agent can get from state to\n",
    "        #    next_state either by the selected action or by slipping.\n",
    "        \n",
    "        # Borders of the lake \n",
    "        border_y, border_x = self.lake.shape\n",
    "        border_x -= 1\n",
    "        border_y -= 1\n",
    "        \n",
    "        # Defining walls as the number of borders each tile collides with.\n",
    "        # Get walls in x and y\n",
    "        walls_y = 1 if state_y == border_y or state_y == 0 else 0\n",
    "        walls_x = 1 if state_x == border_x or state_x == 0 else 0\n",
    "        # Add the number of walls this state collides with x and y\n",
    "        walls   = walls_y + walls_x\n",
    "        \n",
    "        # Transition is from state to next_state, where state is not equal to \n",
    "        # next_state. In other words, the agent is trying to move from the\n",
    "        # current state\n",
    "        if delta_x or delta_y:\n",
    "            # For transitioning from state to the next_state the action provided\n",
    "            # is the correct one. The agent will get to the next state unless it slips\n",
    "            if ((delta_x > 0 and action == right) or (delta_x < 0 and action == left) or\n",
    "                (delta_y > 0 and action == down) or (delta_y < 0 and action == up)):\n",
    "                pt = (1 - self.slip) + (self.slip / self.n_actions)\n",
    "            # For transitioning from state to the next_state the action provided\n",
    "            # is incorrect. The only wat to transition is by slipping. \n",
    "            else:\n",
    "                pt = (self.slip / self.n_actions)\n",
    "        # Transition is to the same state: state == next_state. The agent can remain in \n",
    "        # the same state if it crashes with a wall. The number of walls contribute \n",
    "        # to how probable is to remain in the same state.\n",
    "        elif walls:\n",
    "            # The agent is moving to a wall, it will remain in the same state unless is\n",
    "            # slips to a place without a wall.\n",
    "            if (action == left and state_x == 0) or (action == right and state_x == border_x) or \\\n",
    "               (action == up and state_y == 0) or (action == down and state_y == border_y):\n",
    "                pt = (1 - self.slip) + ((self.slip / self.n_actions) * walls)\n",
    "            # Action is not in a direction of a wall, the agent will remain in the same\n",
    "            # state just if it slips into a wall\n",
    "            else:\n",
    "                pt = (self.slip / self.n_actions) * walls\n",
    "        # Is not necessary to put an else case here, since pt was initialized to 0.0\n",
    "        # If pt has a value of 0.0 at this point means that the agent wanted to remain\n",
    "        # in the same state but there are no walls to crash to remain in the same place. \n",
    "        # So the probability of remaining in the same place is 0 given any action,\n",
    "        # even if the agent slips it will end up moving. \n",
    "                \n",
    "        # Return the probability of moving from state to next_state given an action and\n",
    "        # the probability of slipping\n",
    "        return pt\n",
    "\n",
    "    def r(self, next_state, state, action):\n",
    "        # Reward received by transitioning from state to next_state \n",
    "        # given an action\n",
    "        reward = 0.0\n",
    "        \n",
    "        # The agent receives a reward of 1 upon taking an action in the goal\n",
    "        if state != self.absorbing_state and self.lake_flat[state] == '$':\n",
    "            reward = 1.0\n",
    "    \n",
    "        # In any other case there is no reward\n",
    "        return reward    \n",
    "\n",
    "    def render(self, policy=None, value=None):\n",
    "        if policy is None:\n",
    "            lake = np.array(self.lake_flat)\n",
    "\n",
    "            if self.state < self.absorbing_state:\n",
    "                lake[self.state] = '@'\n",
    "\n",
    "            print(lake.reshape(self.lake.shape))\n",
    "        else:\n",
    "            # UTF-8 arrows look nicer, but cannot be used in LaTeX\n",
    "            # https://www.w3schools.com/charsets/ref_utf_arrows.asp\n",
    "            actions = ['^', '<', '_', '>']\n",
    "\n",
    "            print('Lake:')\n",
    "            print(self.lake)\n",
    "\n",
    "            print('Policy:')\n",
    "            policy = np.array([actions[a] for a in policy[:-1]])\n",
    "            print(policy.reshape(self.lake.shape))\n",
    "\n",
    "            print('Value:')\n",
    "            with _printoptions(precision=3, suppress=True):\n",
    "                print(value[:-1].reshape(self.lake.shape))\n",
    "\n",
    "def play(env):\n",
    "    actions = ['w', 'a', 's', 'd']\n",
    "\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        c = input('\\nMove: ')\n",
    "        if c not in actions:\n",
    "            raise Exception('Invalid action')\n",
    "\n",
    "        state, r, done = env.step(actions.index(c))\n",
    "\n",
    "        env.render()\n",
    "        print('Reward: {0}.'.format(r))\n",
    "        \n",
    "\"\"\"\n",
    "    Definition: Create a random deterministic policy. Returns a numpy array with\n",
    "                the random deterministic policy\n",
    "\"\"\"\n",
    "def random_deterministic_policy(env):\n",
    "    rand_policy = list()\n",
    "    # Get a random action and add them to the policy for each state\n",
    "    [rand_policy.append(np.random.randint(0, env.n_actions)) for i in range(env.n_states)]\n",
    "\n",
    "    return np.array(rand_policy)\n",
    "\n",
    "def policy_evaluation(env, policy, gamma, theta, max_iterations):\n",
    "    value = np.zeros(env.n_states, dtype=np.float32)\n",
    "\n",
    "    # Loop the number of iterations or while our error is greater than\n",
    "    # the tolerance parameter\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        # Go through all tha states\n",
    "        for state in range(env.n_states):\n",
    "            # Store value of the state previous to any modification\n",
    "            v = value[state]\n",
    "            policy_state_product = 0\n",
    "\n",
    "            # We have just one action per state in a deterministic policy, is not necessary\n",
    "            # to loop through all the actions. Leaving this 'for' just to match the policy\n",
    "            # evaluation formula \n",
    "            # Go through all the actions in the policy\n",
    "            for a in range(env.n_actions):\n",
    "                policy_p = 0\n",
    "                # If the action is not part of the policy don't calculate anything\n",
    "                if a == policy[state]:\n",
    "                    policy_p = 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                state_product = 0\n",
    "\n",
    "                # Go through all the possible next states\n",
    "                for state_prime in range(env.n_states):\n",
    "                    # Get probability of transitioning from state to state_prime\n",
    "                    # given action\n",
    "                    pt = env.p(state_prime, state, a)\n",
    "                    # Get reward of transitioning from state to state_prime given\n",
    "                    # action\n",
    "                    rt = env.r(state_prime, state, a)\n",
    "\n",
    "                    state_product += pt * (rt + (gamma * value[state_prime]))\n",
    "\n",
    "                # Store product from all possible action in policy\n",
    "                policy_state_product += state_product * policy_p\n",
    "              \n",
    "            # Update value of this state  \n",
    "            value[state] = policy_state_product\n",
    "\n",
    "            # Calculate the difference between our previous value and the new one\n",
    "            delta = max(delta, abs(v - value[state]))\n",
    "\n",
    "        # If our delta is smaller than the error value theta, stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def policy_improvement(env, value, gamma):\n",
    "    improved_policy = np.zeros(env.n_states, dtype=int)\n",
    "\n",
    "    # Go through all the states\n",
    "    for state in range(env.n_states):\n",
    "        q = list()\n",
    "        \n",
    "        # Go through all the actions\n",
    "        for a in range(env.n_actions):\n",
    "            state_prime_q = 0\n",
    "            \n",
    "            # Go through all next states from the actual state\n",
    "            for state_prime in range(env.n_states):\n",
    "                # Get probability of transitioning from state to state_prime\n",
    "                # given action\n",
    "                pt = env.p(state_prime, state, a)\n",
    "                # Get reward of transitioning from state to state_prime given\n",
    "                # action\n",
    "                rt = env.r(state_prime, state, a)\n",
    "\n",
    "                state_prime_q += pt * (rt + (gamma * value[state_prime]))\n",
    "\n",
    "            # Store value of all actions of possible next states in a list\n",
    "            q.append(state_prime_q)\n",
    "            \n",
    "        q = np.array(q)\n",
    "        \n",
    "        # Get the best action in the current state\n",
    "        improved_policy[state] = np.argmax(q)\n",
    "\n",
    "    # Return improved policy\n",
    "    return improved_policy\n",
    "  \n",
    "def policy_iteration(env, gamma, theta, max_iterations, policy=None):\n",
    "    if policy is None:\n",
    "        policy = np.zeros(env.n_states, dtype=int)\n",
    "    else:\n",
    "        policy = np.array(policy, dtype=int)\n",
    "        \n",
    "    # Iterate the maximum number of iterations or while there are still\n",
    "    # improvements in the policy\n",
    "    for i in range(max_iterations):\n",
    "        prv_policy = policy\n",
    "        #print(\"Iteration \", i)\n",
    "        # Get value of current policy\n",
    "        value = policy_evaluation(env, policy, gamma, theta, max_iterations)\n",
    "        # Improve policy\n",
    "        policy = policy_improvement(env, value, gamma)\n",
    "        \n",
    "        # If policy cannot improve more we have the optimal policy\n",
    "        equal = prv_policy == policy\n",
    "        if not np.any(equal == False):\n",
    "            break\n",
    "\n",
    "    # Return the optimal policy and value of tha policy\n",
    "    return policy, value\n",
    "\n",
    "    \n",
    "def value_iteration(env, gamma, theta, max_iterations, value=None):\n",
    "    if value is None:\n",
    "        value = np.zeros(env.n_states)\n",
    "    else:\n",
    "        value = np.array(value, dtype=np.float32)\n",
    "        \n",
    "    # Keep iterating while we have budget, or if the delta error is reached\n",
    "    for i in range(max_iterations):\n",
    "        #print(\"Iteration \", i)\n",
    "        delta = 0\n",
    "\n",
    "        # Go through all the states\n",
    "        for state in range(env.n_states):\n",
    "            v = value[state]\n",
    "            policy_state_product = list()\n",
    "\n",
    "            # Go through all the actions\n",
    "            for a in range(env.n_actions):\n",
    "                state_product = 0\n",
    "\n",
    "                # Go through all the next states given the current state\n",
    "                for state_prime in range(env.n_states):\n",
    "                    pt = env.p(state_prime, state, a)\n",
    "                    rt = env.r(state_prime, state, a)\n",
    "\n",
    "                    state_product += pt * (rt + (gamma * value[state_prime]))\n",
    "\n",
    "                # Store evaluation of states in a list, we will decide which is\n",
    "                # the best outside this loop\n",
    "                policy_state_product.append(state_product)\n",
    "\n",
    "            # Update value whit best evaluation\n",
    "            policy_state_product = np.array(policy_state_product)\n",
    "            value[state] = np.max(policy_state_product)\n",
    "\n",
    "            # Update delta\n",
    "            delta = max(delta, abs(v - value[state]))\n",
    "\n",
    "        # If our delta is smaller than the error value theta, stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Update policy given the optimal value\n",
    "    policy = policy_improvement(env, value, gamma)\n",
    "\n",
    "    # Return optimal policy and value\n",
    "    return policy, value    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:26:59.925521400Z",
     "start_time": "2023-12-20T21:26:59.899779200Z"
    }
   },
   "id": "c6527c42d481d7df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Testing values of p function, they should match the ones provided in the p.npy file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76db72f106d27220"
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "outputs": [],
   "source": [
    "# We have 17 * 17 * 4 shape of the matrix, absorving state is the last one\n",
    "# This means that we can move betweeen the 16 states, and wwe can transition to the absorbing state that is 17\n",
    "# next_state, state, action\n",
    "lake_p = np.load(\"p.npy\")\n",
    "seed = 0\n",
    "gamma = 0.9\n",
    "\n",
    "# Small frozen lake array\n",
    "lake =  [['&', '.', '.', '.'],\n",
    "        ['.', '#', '.', '#'],\n",
    "        ['.', '.', '.', '#'],\n",
    "        ['#', '.', '.', '$']]\n",
    "\n",
    "# Big lake\n",
    "#lake = [['&', '.', '.', '.', '.', '.', '.', '.'],\n",
    "#        ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "#        ['.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "#        ['.', '.', '.', '.', '.', '#', '.', '.'],\n",
    "#        ['.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "#        ['.', '#', '#', '.', '.', '.', '#', '.'],\n",
    "#        ['.', '#', '.', '.', '#', '.', '#', '.'],\n",
    "#        ['.', '.', '.', '#', '.', '.', '.', '$']]\n",
    "\n",
    "# Create small frozen lake environment\n",
    "env = FrozenLake(lake, slip=0.1, max_steps=16, seed=seed)\n",
    "\n",
    "# Create an array of transitioning to all the states given all\n",
    "# the actions in the lake. Using this array to compare it to the\n",
    "# one provided in the assignment that was loaded at the begining\n",
    "# of this document \n",
    "probb = list()\n",
    "for nextState in range (env.n_states):\n",
    "    for sttate in range (env.n_states):\n",
    "        for act in range (env.n_actions):\n",
    "            probb.append((env.p(nextState, sttate, act)))\n",
    "\n",
    "lake = np.array(lake)\n",
    "lake_flat = lake.reshape(-1)\n",
    "\n",
    "# Convert list to numpy array\n",
    "probb = np.array(probb)\n",
    "# Give correct shape, trusting in reshapes black magic\n",
    "probb = probb.reshape(lake.size + 1, lake.size + 1, 4)\n",
    "\n",
    "# Validate that the array is the same\n",
    "#equal = probb == lake_p\n",
    "# If there is no False, means that all probabilities match\n",
    "# with the probabilities given in the assignment.\n",
    "#print(np.where(equal == False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:27:00.526838500Z",
     "start_time": "2023-12-20T21:27:00.495152500Z"
    }
   },
   "id": "1c3d7ed2c316b52d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Testing of: Tabular model-based reinforcement learning\n",
    "**Testing of:** Policy evaluation, policy improvement, policy iteration and value iteration<br>\n",
    "**Hint from document:** A *deterministic* policy may be represented by an array that contains the action prescribed for each state  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d55fc42012e2b86d"
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Model-based algorithms\n",
      "\n",
      "## Policy iteration\n",
      "Lake:\n",
      "[['&' '.' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '#']\n",
      " ['#' '.' '.' '$']]\n",
      "Policy:\n",
      "[['_' '>' '_' '<']\n",
      " ['_' '^' '_' '^']\n",
      " ['>' '_' '_' '^']\n",
      " ['^' '>' '>' '^']]\n",
      "Value:\n",
      "[[0.455 0.504 0.579 0.505]\n",
      " [0.508 0.    0.653 0.   ]\n",
      " [0.584 0.672 0.768 0.   ]\n",
      " [0.    0.771 0.887 1.   ]]\n",
      "\n",
      "## Value iteration\n",
      "Lake:\n",
      "[['&' '.' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '#']\n",
      " ['#' '.' '.' '$']]\n",
      "Policy:\n",
      "[['_' '>' '_' '<']\n",
      " ['_' '^' '_' '^']\n",
      " ['>' '_' '_' '^']\n",
      " ['^' '>' '>' '^']]\n",
      "Value:\n",
      "[[0.455 0.504 0.579 0.505]\n",
      " [0.508 0.    0.653 0.   ]\n",
      " [0.584 0.672 0.768 0.   ]\n",
      " [0.    0.771 0.887 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Taken from document in the main function side\n",
    "theta = 0.001\n",
    "max_iterations = 128\n",
    "\n",
    "print('# Model-based algorithms')\n",
    "\n",
    "print('')\n",
    "\n",
    "print('## Policy iteration')\n",
    "policy, value = policy_iteration(env, gamma, theta=0.001, max_iterations=128)\n",
    "env.render(policy, value)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('## Value iteration')\n",
    "policy, value = value_iteration(env, gamma, theta=0.001, max_iterations=128)\n",
    "env.render(policy, value)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T21:27:02.177065900Z",
     "start_time": "2023-12-20T21:27:01.862185200Z"
    }
   },
   "id": "590e6f6c1b504cb2"
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T20:02:24.084612800Z",
     "start_time": "2023-12-17T20:02:24.067598700Z"
    }
   },
   "id": "84fbc3881304a3f"
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T17:27:17.640329800Z",
     "start_time": "2023-12-17T17:27:17.615336300Z"
    }
   },
   "id": "ead7e60b86563f50"
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T17:27:17.800137900Z",
     "start_time": "2023-12-17T17:27:17.772103200Z"
    }
   },
   "id": "8133fe171cab6bae"
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T17:27:17.976955600Z",
     "start_time": "2023-12-17T17:27:17.933628300Z"
    }
   },
   "id": "7bbe44efb5c6de02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7fd85b3ba53f44b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
